{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d996b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best individual is [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], (10.0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from deap import base, creator, tools\n",
    "\n",
    "# Define the fitness function\n",
    "def evalOneMax(individual):\n",
    "    return (sum(individual),)  # Sum the number of 1s in the individual\n",
    "\n",
    "# Set up the environment\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Objective: maximize the fitness\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, 10)  # 10 genes per individual\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", evalOneMax)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)  # 10% mutation probability\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Create a population and evaluate it\n",
    "population = toolbox.population(n=50)  # 50 individuals in the population\n",
    "NGEN = 10  # Number of generations\n",
    "\n",
    "for gen in range(NGEN):\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        toolbox.mate(child1, child2)\n",
    "        del child1.fitness.values\n",
    "        del child2.fitness.values\n",
    "\n",
    "    for mutant in offspring:\n",
    "        toolbox.mutate(mutant)\n",
    "        del mutant.fitness.values\n",
    "\n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    for ind in invalid_ind:\n",
    "        ind.fitness.values = toolbox.evaluate(ind)\n",
    "\n",
    "    population = offspring\n",
    "\n",
    "# Extracting and printing the best result\n",
    "best_ind = tools.selBest(population, 1)[0]\n",
    "print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412c7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRWeatherAnalysis(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_temps,\n",
    "                   reducer=self.reducer_get_max_temp),\n",
    "            MRStep(reducer=self.reducer_find_max_temp_year)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_temps(self, _, line):\n",
    "        # Split the line into components\n",
    "        parts = line.split(',')\n",
    "        try:\n",
    "            year = parts[0]\n",
    "            temp = float(parts[1])\n",
    "            yield year, temp\n",
    "        except ValueError:\n",
    "            pass  # ignore lines with invalid data\n",
    "\n",
    "    def reducer_get_max_temp(self, year, temps):\n",
    "        # Send the max temperature for each year to the same reducer\n",
    "        yield None, (max(temps), year)\n",
    "\n",
    "    def reducer_find_max_temp_year(self, _, year_temp_pairs):\n",
    "        # Find the year with the maximum temperature\n",
    "        yield max(year_temp_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73326a33",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m mr_job \u001b[38;5;241m=\u001b[39m MRWeatherAnalysis(args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweather.csv\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Ensure 'weather_data.csv' is the path to your data file\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Execute the job\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmr_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m     39\u001b[0m     runner\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Print results directly\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/mrjob/job.py:704\u001b[0m, in \u001b[0;36mMRJob.make_runner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_runner() was called with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m probably means you tried to use it from\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m __main__, which doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m w)\n\u001b[1;32m    703\u001b[0m runner_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner_class()\n\u001b[0;32m--> 704\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runner_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# screen out most false-ish args so that it's readable\u001b[39;00m\n\u001b[1;32m    707\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaking runner: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, ...)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    708\u001b[0m     runner_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (k, v)\n\u001b[1;32m    710\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    711\u001b[0m               \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, [], {}))))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/mrjob/job.py:727\u001b[0m, in \u001b[0;36mMRJob._runner_kwargs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_runner_kwargs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;124;03m\"\"\"If we're building an inline or Spark runner,\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    include mrjob_cls in kwargs.\"\"\"\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m combine_dicts(\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_option_kwargs(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;66;03m# don't screen out irrelevant opts (see #1898)\u001b[39;00m\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs_from_switches(\u001b[38;5;28mset\u001b[39m(_RUNNER_OPTS)),\n\u001b[0;32m--> 727\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_job_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    728\u001b[0m     )\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner_class()\u001b[38;5;241m.\u001b[39malias \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    731\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(mrjob_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/mrjob/job.py:246\u001b[0m, in \u001b[0;36mMRJob._job_kwargs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"Keyword arguments to the runner class that can be specified\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mby the job/launcher itself.\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# use the most basic combiners; leave magic like resolving paths\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# and blanking out jobconf values to the runner\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# command-line has the final say on jobconf and libjars\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     jobconf\u001b[38;5;241m=\u001b[39mcombine_dicts(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobconf(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mjobconf),\n\u001b[1;32m    245\u001b[0m     libjars\u001b[38;5;241m=\u001b[39mcombine_lists(\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibjars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mlibjars),\n\u001b[1;32m    247\u001b[0m     partitioner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitioner(),\n\u001b[1;32m    248\u001b[0m     sort_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_values(),\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# TODO: should probably put self.options last below for consistency\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     upload_archives\u001b[38;5;241m=\u001b[39mcombine_lists(\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mupload_archives, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchives()),\n\u001b[1;32m    252\u001b[0m     upload_dirs\u001b[38;5;241m=\u001b[39mcombine_lists(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mupload_dirs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirs()),\n\u001b[1;32m    254\u001b[0m     upload_files\u001b[38;5;241m=\u001b[39mcombine_lists(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mupload_files, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles()),\n\u001b[1;32m    256\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/mrjob/job.py:1373\u001b[0m, in \u001b[0;36mMRJob.libjars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlibjars\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optional list of paths of jar files to run our job with using\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03m    Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;124;03m    is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;124;03m       ``--libjars`` option\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1373\u001b[0m     script_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmr_job_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     paths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;66;03m# libjar paths will eventually be combined with combine_path_lists,\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;66;03m# which will expand environment variables. We don't want to assume\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# with /), but we also don't want to expand environment variables\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;66;03m# prematurely.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/posixpath.py:152\u001b[0m, in \u001b[0;36mdirname\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdirname\u001b[39m(p):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the directory component of a pathname\"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(p)\n\u001b[1;32m    154\u001b[0m     i \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mrfind(sep) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary modules from mrjob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# Define the MapReduce job class\n",
    "class MRWeatherAnalysis(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_temps,\n",
    "                   reducer=self.reducer_get_max_temp),\n",
    "            MRStep(reducer=self.reducer_find_max_temp_year)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_temps(self, _, line):\n",
    "        # Split the line into components assuming the format \"Year,Temperature\"\n",
    "        parts = line.split(',')\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                year = parts[0].strip()\n",
    "                temp = float(parts[1].strip())\n",
    "                yield year, temp\n",
    "            except ValueError:\n",
    "                pass  # ignore lines with invalid data\n",
    "\n",
    "    def reducer_get_max_temp(self, year, temps):\n",
    "        # Yield the max temperature for each year\n",
    "        yield None, (max(temps), year)\n",
    "\n",
    "    def reducer_find_max_temp_year(self, _, year_temp_pairs):\n",
    "        # Find and yield the year with the maximum temperature\n",
    "        yield 'Hottest Year', max(year_temp_pairs, key=lambda x: x[0])\n",
    "\n",
    "# Create an instance of the job\n",
    "mr_job = MRWeatherAnalysis(args=['weather.csv'])  # Ensure 'weather_data.csv' is the path to your data file\n",
    "\n",
    "# Execute the job\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    \n",
    "    # Print results directly\n",
    "    for line in runner.stream_output():\n",
    "        key, value = mr_job.parse_output_line(line)\n",
    "        print(f'{key}: Year {value[1]} with max temp {value[0]}°C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9633c9af",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m mr_job \u001b[38;5;241m=\u001b[39m MRWeatherAnalysis(args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Simulate the input file as in-memory string stream\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmr_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Start the context for the MapReduce job\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     runner\u001b[38;5;241m.\u001b[39mrun(stdin\u001b[38;5;241m=\u001b[39mio\u001b[38;5;241m.\u001b[39mStringIO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data)))\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Print results directly\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/mrjob/job.py:704\u001b[0m, in \u001b[0;36mMRJob.make_runner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_runner() was called with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m probably means you tried to use it from\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m __main__, which doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m w)\n\u001b[1;32m    703\u001b[0m runner_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner_class()\n\u001b[0;32m--> 704\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runner_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# screen out most false-ish args so that it's readable\u001b[39;00m\n\u001b[1;32m    707\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaking runner: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, ...)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    708\u001b[0m     runner_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (k, v)\n\u001b[1;32m    710\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    711\u001b[0m               \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, [], {}))))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/mrjob/job.py:727\u001b[0m, in \u001b[0;36mMRJob._runner_kwargs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_runner_kwargs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;124;03m\"\"\"If we're building an inline or Spark runner,\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    include mrjob_cls in kwargs.\"\"\"\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m combine_dicts(\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_option_kwargs(),\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;66;03m# don't screen out irrelevant opts (see #1898)\u001b[39;00m\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs_from_switches(\u001b[38;5;28mset\u001b[39m(_RUNNER_OPTS)),\n\u001b[0;32m--> 727\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_job_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    728\u001b[0m     )\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner_class()\u001b[38;5;241m.\u001b[39malias \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    731\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(mrjob_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/mrjob/job.py:246\u001b[0m, in \u001b[0;36mMRJob._job_kwargs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"Keyword arguments to the runner class that can be specified\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mby the job/launcher itself.\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# use the most basic combiners; leave magic like resolving paths\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# and blanking out jobconf values to the runner\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# command-line has the final say on jobconf and libjars\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     jobconf\u001b[38;5;241m=\u001b[39mcombine_dicts(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobconf(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mjobconf),\n\u001b[1;32m    245\u001b[0m     libjars\u001b[38;5;241m=\u001b[39mcombine_lists(\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibjars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mlibjars),\n\u001b[1;32m    247\u001b[0m     partitioner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitioner(),\n\u001b[1;32m    248\u001b[0m     sort_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_values(),\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# TODO: should probably put self.options last below for consistency\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     upload_archives\u001b[38;5;241m=\u001b[39mcombine_lists(\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mupload_archives, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchives()),\n\u001b[1;32m    252\u001b[0m     upload_dirs\u001b[38;5;241m=\u001b[39mcombine_lists(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mupload_dirs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirs()),\n\u001b[1;32m    254\u001b[0m     upload_files\u001b[38;5;241m=\u001b[39mcombine_lists(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mupload_files, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles()),\n\u001b[1;32m    256\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/mrjob/job.py:1373\u001b[0m, in \u001b[0;36mMRJob.libjars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlibjars\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optional list of paths of jar files to run our job with using\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03m    Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;124;03m    is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;124;03m       ``--libjars`` option\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1373\u001b[0m     script_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmr_job_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     paths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;66;03m# libjar paths will eventually be combined with combine_path_lists,\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;66;03m# which will expand environment variables. We don't want to assume\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# with /), but we also don't want to expand environment variables\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;66;03m# prematurely.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/posixpath.py:152\u001b[0m, in \u001b[0;36mdirname\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdirname\u001b[39m(p):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the directory component of a pathname\"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(p)\n\u001b[1;32m    154\u001b[0m     i \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mrfind(sep) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from mrjob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import io\n",
    "\n",
    "# Define the MapReduce job class\n",
    "class MRWeatherAnalysis(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_temps,\n",
    "                   reducer=self.reducer_get_max_temp),\n",
    "            MRStep(reducer=self.reducer_find_max_temp_year)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_temps(self, _, line):\n",
    "        # Split the line into components assuming the format \"Year,Temperature\"\n",
    "        parts = line.split(',')\n",
    "        try:\n",
    "            year = parts[0].strip()\n",
    "            temp = float(parts[1].strip())\n",
    "            yield year, temp\n",
    "        except ValueError:\n",
    "            pass  # ignore lines with invalid data\n",
    "\n",
    "    def reducer_get_max_temp(self, year, temps):\n",
    "        # Yield the max temperature for each year\n",
    "        yield None, (max(temps), year)\n",
    "\n",
    "    def reducer_find_max_temp_year(self, _, year_temp_pairs):\n",
    "        # Find and yield the year with the maximum temperature\n",
    "        yield 'Hottest Year', max(year_temp_pairs, key=lambda x: x[0])\n",
    "\n",
    "# Generate sample data\n",
    "data = [\n",
    "    '1990,15.6',\n",
    "    '1991,15.9',\n",
    "    '1990,16.2',\n",
    "    '1992,15.2',\n",
    "    '1991,17.5',\n",
    "    '1992,18.1',\n",
    "    '1993,14.3'\n",
    "]\n",
    "\n",
    "# Create an instance of the job\n",
    "mr_job = MRWeatherAnalysis(args=['-'])\n",
    "\n",
    "# Simulate the input file as in-memory string stream\n",
    "with mr_job.make_runner() as runner:\n",
    "    # Start the context for the MapReduce job\n",
    "    runner.run(stdin=io.StringIO('\\n'.join(data)))\n",
    "    \n",
    "    # Print results directly\n",
    "    for line in runner.stream_output():\n",
    "        key, value = mr_job.parse_output_line(line)\n",
    "        print(f'{key}: Year {value[1]} with max temp {value[0]}°C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b534dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters: [2.00137684e+02 5.40950559e-01 3.45301289e+02 1.99384196e+03]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "NUM_PARAMETERS = 4  # Number of parameters (e.g., temperature, feed rate, etc.)\n",
    "POPULATION_SIZE = 50  # Number of individuals in the population\n",
    "MAX_GENERATIONS = 100  # Number of generations\n",
    "MUTATION_RATE = 0.01  # Probability of mutation\n",
    "PARAMETER_RANGES = np.array([[100, 300],  # Inlet air temperature range\n",
    "                             [0.1, 1.0],  # Feed rate range\n",
    "                             [200, 500],  # Air flow rate range\n",
    "                             [1000, 3000]])  # Atomizer speed range\n",
    "\n",
    "# Neural network simulation\n",
    "def predict_nn(parameters):\n",
    "    # Simulate NN predictions. In practice, you would replace this with actual model predictions.\n",
    "    # For simplicity, let's assume better performance is when parameters are closer to the middle of their range.\n",
    "    ideal = np.mean(PARAMETER_RANGES, axis=1)\n",
    "    return -np.sum((parameters - ideal) ** 2, axis=1)  # Negative squared distance from the ideal point\n",
    "\n",
    "# Genetic Algorithm Components\n",
    "def initialize_population(size):\n",
    "    return np.random.uniform(low=PARAMETER_RANGES[:, 0], high=PARAMETER_RANGES[:, 1], size=(size, NUM_PARAMETERS))\n",
    "\n",
    "def evaluate_fitness(population):\n",
    "    return predict_nn(population)\n",
    "\n",
    "def select(population, fitness, num_parents):\n",
    "    # Tournament selection\n",
    "    parents = np.zeros((num_parents, NUM_PARAMETERS))\n",
    "    for i in range(num_parents):\n",
    "        random_ids = np.random.randint(0, len(population), 4)\n",
    "        best_id = random_ids[np.argmax(fitness[random_ids])]\n",
    "        parents[i] = population[best_id]\n",
    "    return parents\n",
    "\n",
    "def crossover(parents):\n",
    "    offspring = np.zeros((POPULATION_SIZE, NUM_PARAMETERS))\n",
    "    crossover_point = np.random.randint(1, NUM_PARAMETERS-1)\n",
    "    for i in range(POPULATION_SIZE):\n",
    "        parent1_index = i % parents.shape[0]\n",
    "        parent2_index = (i+1) % parents.shape[0]\n",
    "        offspring[i, :crossover_point] = parents[parent1_index, :crossover_point]\n",
    "        offspring[i, crossover_point:] = parents[parent2_index, crossover_point:]\n",
    "    return offspring\n",
    "\n",
    "def mutate(offspring):\n",
    "    for idx in range(offspring.shape[0]):\n",
    "        for jdx in range(NUM_PARAMETERS):\n",
    "            if np.random.rand() < MUTATION_RATE:\n",
    "                random_value = np.random.uniform(PARAMETER_RANGES[jdx, 0], PARAMETER_RANGES[jdx, 1])\n",
    "                offspring[idx, jdx] = random_value\n",
    "    return offspring\n",
    "\n",
    "def genetic_algorithm():\n",
    "    population = initialize_population(POPULATION_SIZE)\n",
    "    for generation in range(MAX_GENERATIONS):\n",
    "        fitness = evaluate_fitness(population)\n",
    "        parents = select(population, fitness, POPULATION_SIZE // 2)\n",
    "        offspring = crossover(parents)\n",
    "        population = mutate(offspring)\n",
    "    best_index = np.argmax(evaluate_fitness(population))\n",
    "    return population[best_index]\n",
    "\n",
    "# Run the genetic algorithm\n",
    "best_parameters = genetic_algorithm()\n",
    "print(\"Optimized Parameters:\", best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c59e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
